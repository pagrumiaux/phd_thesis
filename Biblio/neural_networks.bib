
@article{purwins_deep_2019,
	title = {Deep learning for audio signal processing},
	volume = {13},
	issn = {1932-4553, 1941-0484},
	doi = {10.1109/JSTSP.2019.2908700},
	abstract = {Given the recent surge in developments of deep learning, this article provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross-fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e. audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan and Chang, Shuo-yiin and Sainath, Tara},
	month = apr,
	year = {2019},
	keywords = {lu},
	pages = {206--219},
	file = {arXiv\:1905.00078 PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\KHUSBDWQ\\Purwins et al. - 2019 - Deep Learning for Audio Signal Processing.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\YDMQEZL4\\1905.html:text/html},
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: a generative model for raw audio},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	language = {en},
	journal = {arXiv:1609.03499},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, lu},
	file = {Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\Z8FP47L6\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	journal = {arXiv:1712.01815},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, lu},
	file = {arXiv\:1712.01815 PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\U2H9ZHWU\\Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\L98AT8LM\\1712.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is all you need},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	booktitle = {Conference on {Neural} {Information} {Processing} {System}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	keywords = {lu},
	pages = {5998--6008},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\FZV3RM7K\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An image is worth 16x16 words: transformers for image recognition at scale},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = may,
	year = {2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\X5F9AGFT\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\VYE2PTBR\\2010.html:text/html},
}

@inproceedings{bahdanau_neural_2015,
	title = {Neural machine translation by jointly learning to align and translate},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\LS2UJKU7\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\USH3QQUH\\1409.html:text/html},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	volume = {1},
	issn = {0899-7667},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	journal = {Neural Computation},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
	year = {1989},
	pages = {541--551},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\XCPKG86P\\6795724.html:text/html;LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\8Y7SFPWP\\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep residual learning for image recognition},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\UMNDRG38\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\Y8DRB85Z\\He_Deep_Residual_Learning_CVPR_2016_paper.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\XQVGS4MG\\Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\ZRS6CMGL\\Long-Short-Term-Memory.html:text/html},
}

@inproceedings{cho_learning_2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\9BIYGBMV\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\TDFWZ596\\1406.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep learning},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
	language = {en},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1989},
	pages = {359--366},
	file = {Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\AJUVVXKQ\\Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:application/pdf},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	issn = {0028-0836, 1476-4687},
	doi = {10.1038/s41586-021-03819-2},
	language = {en},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = jul,
	year = {2021},
	pages = {1--11},
	file = {Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\G4CPRQGG\\Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}

@article{hinton_deep_2012,
	title = {Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups},
	volume = {29},
	issn = {1558-0792},
	shorttitle = {Deep {Neural} {Networks} for {Acoustic} {Modeling} in {Speech} {Recognition}},
	doi = {10.1109/MSP.2012.2205597},
	abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
	month = nov,
	year = {2012},
	pages = {82--97},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\CJ5U5PI3\\6296526.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\H56JNARD\\Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf:application/pdf},
}

@article{hennequin_spleeter_2020,
	title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {Spleeter},
	doi = {10.21105/joss.02154},
	abstract = {Hennequin et al., (2020). Spleeter: a fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software, 5(50), 2154, https://doi.org/10.21105/joss.02154},
	language = {en},
	number = {50},
	journal = {Journal of Open Source Software},
	author = {Hennequin, Romain and Khlif, Anis and Voituret, Felix and Moussallam, Manuel},
	month = jun,
	year = {2020},
	pages = {2154},
	file = {Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\YIIR58LG\\Hennequin et al. - 2020 - Spleeter a fast and efficient music source separa.pdf:application/pdf;Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\E94D5STG\\joss.html:text/html},
}

@inproceedings{hadjeres_deepbach_2017,
	title = {{DeepBach}: a steerable model for bach chorales generation},
	shorttitle = {{DeepBach}},
	abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
	month = aug,
	year = {2017},
	pages = {1362--1371},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\UZICLUW5\\Hadjeres et al. - 2017 - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\KZH3UI2U\\1612.html:text/html},
}

@article{mcculloch_logicial_1943,
	title = {A logicial calculus of the ideas immanent in nervous activity},
	volume = {5},
	language = {en},
	journal = {The Bulletin of Mathematical Biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	pages = {115--133},
	file = {Mcculloch and Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\6GYGXDY7\\Mcculloch and Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:application/pdf},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overﬁtting},
	volume = {15},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	month = jun,
	year = {2014},
	pages = {1929--1958},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\BSJ556WW\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@inproceedings{yu_multi-scale_2016,
	title = {Multi-scale context aggregation by dilated convolutions},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Yu, Fisher and Koltun, Vladlen},
	year = {2016},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\VV42T5WA\\Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\QBHE2SFI\\1511.html:text/html},
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1941-0476},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, Mike and Paliwal, Kuldip K.},
	month = nov,
	year = {1997},
	pages = {2673--2681},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\W5AG93DU\\650093.html:text/html;Submitted Version:C\:\\Users\\RQML4978\\Zotero\\storage\\J6VWWXND\\Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf},
}

@inproceedings{luong_effective_2015,
	title = {Effective approaches to attention-based neural machine translation},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	pages = {1412--1421},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\3RB4RDTT\\Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\9FK8RWC4\\1508.html:text/html},
}

@inproceedings{brown_language_2020,
	title = {Language models are few-shot learners},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	booktitle = {Conference on {Neural} {Information} {Processing} {System}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\VHD56GML\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{devlin_bert_2019,
	title = {{BERT}: pre-training of deep bidirectional transformers for language understanding},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	journal = {arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\47F4AWDD\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{huang_densely_2017,
	title = {Densely connected convolutional networks},
	doi = {10.1109/CVPR.2017.243},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	pages = {2261--2269},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\LZKJGXYN\\8099726.html:text/html;Submitted Version:C\:\\Users\\RQML4978\\Zotero\\storage\\HGLV7WN7\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:application/pdf},
}
