
@book{vincent_audio_2018,
	title = {Audio source separation and speech enhancement},
	publisher = {John Wiley \& Sons},
	author = {Vincent, Emmanuel and Virtanen, Tuomas and Gannot, Shannon},
	year = {2018},
	keywords = {non-lu},
	file = {Vincent et al. - 2018 - Audio Source Separation and Speech Enhancement.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\9Y4MT6VY\\Vincent et al. - 2018 - Audio Source Separation and Speech Enhancement.pdf:application/pdf},
}

@article{anguera_speaker_2012,
	title = {Speaker diarization: a review of recent research},
	volume = {20},
	issn = {1558-7916},
	shorttitle = {Speaker {Diarization}},
	doi = {10.1109/TASL.2011.2125954},
	abstract = {Speaker diarization is the task of determining “who spoke when?” in an audio or video recording that contains an unknown amount of speech and also an unknown number of speakers. Initially, it was proposed as a research topic related to automatic speech recognition, where speaker diarization serves as an upstream processing step. Over recent years, however, speaker diarization has become an important key technology for many tasks, such as navigation, retrieval, or higher level inference on audio data. Accordingly, many important improvements in accuracy and robustness have been reported in journals and conferences in the area. The application domains, from broadcast news, to lectures and meetings, vary greatly and pose different problems, such as having access to multiple microphones and multimodal information or overlapping speech. The most recent review of existing technology dates back to 2006 and focuses on the broadcast news domain. In this paper, we review the current state-of-the-art, focusing on research developed since 2006 that relates predominantly to speaker diarization for conference meetings. Finally, we present an analysis of speaker diarization performance as reported through the NIST Rich Transcription evaluations on meeting data and identify important areas for future research.},
	number = {2},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Anguera, Xavier and Bozonnet, Simon and Evans, Nicholas and Fredouille, Corinne and Friedland, Gerald and Vinyals, Oriol},
	month = feb,
	year = {2012},
	keywords = {Acoustics, Adaptation models, audio data, audio recording, audio signal processing, automatic speech recognition, broadcast news, conference meetings, Data models, information resources, Meetings, Microphones, multimodal information, NIST, NIST Rich Transcription evaluations, non-lu, rich transcription, speaker diarization, speaker recognition, Speech, speech overlapping, Speech recognition, teleconferencing, television broadcasting, upstream processing, video recording},
	pages = {356--370},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\LY5XV48K\\6135543.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\55LN3XWQ\\Anguera et al. - 2012 - Speaker Diarization A Review of Recent Research.pdf:application/pdf},
}

@inproceedings{daniel_time_2020,
	title = {Time domain velocity vector for retracing the multipath propagation},
	isbn = {978-1-5090-6631-5},
	doi = {10.1109/ICASSP40776.2020.9054561},
	abstract = {We propose a conceptually and computationally simple form of sound velocity that offers a readable view of the interference between direct and indirect sound waves. Unlike most approaches in the literature, it jointly exploits both active and reactive sound intensity measurements, as typically derived from a ﬁrst order ambisonics recording. This representation has a potential both as a valuable tool for directly analyzing sound multipath propagation, as well as being a new spatial feature format for machine learning algorithms in audio and acoustics. As a showcase, we demonstrate that the Directionof-Arrival and the range of a sound source can be estimated as a development of this approach. To the best knowledge of the authors, this is the ﬁrst time that range is estimated from an ambisonics recording.},
	language = {en},
	urldate = {2020-05-25},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Daniel, Jerome and Kitic, Srdan},
	year = {2020},
	keywords = {lu},
	pages = {421--425},
	file = {Daniel et Kitic - 2020 - Time Domain Velocity Vector for Retracing the Mult.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\7RWFEP3N\\Daniel et Kitic - 2020 - Time Domain Velocity Vector for Retracing the Mult.pdf:application/pdf},
}

@inproceedings{grumiaux_multichannel_2020,
	title = {Multichannel {CRNN} for speaker counting: an analysis of performance},
	shorttitle = {Multichannel {CRNN} for {Speaker} {Counting}},
	abstract = {Speaker counting is the task of estimating the number of people that are simultaneously speaking in an audio recording. For several audio processing tasks such as speaker diarization, separation, localization and tracking, knowing the number of speakers at each timestep is a prerequisite, or at least it can be a strong advantage, in addition to enabling a low latency processing. In a previous work, we addressed the speaker counting problem with a multichannel convolutional recurrent neural network which produces an estimation at a short-term frame resolution. In this work, we show that, for a given frame, there is an optimal position in the input sequence for best prediction accuracy. We empirically demonstrate the link between that optimal position, the length of the input sequence and the size of the convolutional ﬁlters.},
	language = {en},
	urldate = {2021-03-15},
	booktitle = {Forum {Acusticum}},
	author = {Grumiaux, Pierre-Amaury and Kitic, Srdan and Girin, Laurent and Guérin, Alexandre},
	year = {2020},
	keywords = {lu},
	file = {Grumiaux et al. - 2021 - Multichannel CRNN for Speaker Counting an Analysi.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\4CTMFZQN\\Grumiaux et al. - 2021 - Multichannel CRNN for Speaker Counting an Analysi.pdf:application/pdf},
}

@inproceedings{grumiaux_improved_2021,
	title = {Improved feature extraction for {CRNN}-based multiple sound source localization},
	abstract = {In this work, we propose to extend a state-of-the-art
multi-source localization system based on a convolutional recurrent neural network and Ambisonics signals. We significantly
improve the performance of the baseline network by changing
the layout between convolutional and pooling layers. We propose
several configurations with more convolutional layers and smaller
pooling sizes in-between, so that less information is lost across
the layers, leading to a better feature extraction. In parallel, we
test the system’s ability to localize up to 3 sources, in which case
the improved feature extraction provides the most significant
boost in accuracy. We evaluate and compare these improved
configurations on synthetic and real-world data. The obtained
results show a quite substantial improvement of the multiple
sound source localization performance over the baseline network.},
	language = {en},
	booktitle = {European {Signal} {Processing} {Conference}},
	author = {Grumiaux, Pierre-Amaury and Kitic, Srdan and Girin, Laurent and Guérin, Alexandre},
	year = {2021},
	keywords = {lu},
	file = {Grumiaux et al. - Improved feature extraction for CRNN-based multipl.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\37MNRQRR\\Grumiaux et al. - Improved feature extraction for CRNN-based multipl.pdf:application/pdf},
}

@article{perotin_crnn-based_2019,
	title = {{CRNN}-based multiple {DoA} estimation using acoustic intensity features for {Ambisonics} recordings},
	volume = {13},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2019.2900164},
	abstract = {Localizing audio sources is challenging in real reverberant environments, especially when several sources are active. We propose to use a neural network built from stacked convolutional and recurrent layers in order to estimate the directions of arrival of multiple sources from a first-order Ambisonics recording. It returns the directions of arrival over a discrete grid of a known number of sources. We propose to use features derived from the acoustic intensity vector as inputs. We analyze the behavior of the neural network by means of a visualization technique called layerwise relevance propagation. This analysis highlights which parts of the input signal are relevant in a given situation. We also conduct experiments to evaluate the performance of our system in various environments, from simulated rooms to real recordings, with one or two speech sources. The results show that the proposed features significantly improve performances with respect to raw Ambisonics inputs.},
	number = {1},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Perotin, Lauréline and Serizel, Romain and Vincent, Emmanuel and Guérin, Alexandre},
	month = mar,
	year = {2019},
	keywords = {lu},
	pages = {22--33},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\IFFJIMX5\\8643769.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\9RS53UMH\\Perotin et al. - 2019 - CRNN-Based Multiple DoA Estimation Using Acoustic .pdf:application/pdf;Perotin - CRNN-based multiple DoA estimation using Ambisonic.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\TPWGMJPJ\\Perotin - CRNN-based multiple DoA estimation using Ambisonic.pdf:application/pdf},
}

@inproceedings{grumiaux_high-resolution_2020,
	address = {Amsterdam, Netherlands},
	title = {High-resolution speaker counting in reverberant rooms using {CRNN} with {Ambisonics} features},
	isbn = {978-90-827970-5-3},
	doi = {10.23919/Eusipco47968.2020.9287637},
	abstract = {Speaker counting is the task of estimating the number of people that are simultaneously speaking in an audio recording. For several audio processing tasks such as speaker diarization, separation, localization and tracking, knowing the number of speakers at each timestep is a prerequisite, or at least it can be a strong advantage, in addition to enabling a low latency processing. For that purpose, we address the speaker counting problem with a multichannel convolutional recurrent neural network which produces an estimation at a short-term frame resolution. We trained the network to predict up to 5 concurrent speakers in a multichannel mixture, with simulated data including many different conditions in terms of source and microphone positions, reverberation, and noise. The network can predict the number of speakers with good accuracy at frame resolution.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {European {Signal} {Processing} {Conference}},
	author = {Grumiaux, Pierre-Amaury and Kitic, Srdan and Girin, Laurent and Guerin, Alexandre},
	year = {2020},
	keywords = {lu},
	file = {Grumiaux et al. - 2021 - High-Resolution Speaker Counting in Reverberant Ro.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\YY2DZ9LC\\Grumiaux et al. - 2021 - High-Resolution Speaker Counting in Reverberant Ro.pdf:application/pdf},
}

@article{wang_supervised_2018,
	title = {Supervised speech separation based on deep learning: an overview},
	volume = {26},
	issn = {2329-9304},
	shorttitle = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}},
	doi = {10.1109/TASLP.2018.2842159},
	abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
	number = {10},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, DeLiang and Chen, Jitong},
	month = oct,
	year = {2018},
	pages = {1702--1726},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\AVQ23UJ7\\8369155.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\2KIQI96G\\Wang et Chen - 2018 - Supervised Speech Separation Based on Deep Learnin.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep learning},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{grumiaux_saladnet_2021,
	title = {{SALADnet}: self-attentive multisource localization in the {Ambisonics} domain},
	abstract = {In this work, we propose a novel self-attention based neural network for robust multi-speaker localization from Ambisonics recordings. Starting from a state-of-the-art convolutional recurrent neural network, we investigate the beneﬁt of replacing the recurrent layers by self-attention encoders, inherited from the Transformer architecture. We evaluate these models on synthetic and real-world data, with up to 3 simultaneous speakers. The obtained results indicate that the majority of the proposed architectures either perform on par, or outperform the CRNN baseline, especially in the multisource scenario. Moreover, by avoiding the recurrent layers, the proposed models lend themselves to parallel computing, which is shown to produce considerable savings in execution time.},
	language = {en},
	journal = {Accepted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
	author = {Grumiaux, Pierre-Amaury and Kitic, Srdan and Srivastava, Prerak and Girin, Laurent and Guérin, Alexandre},
	year = {2021},
	keywords = {lu},
	file = {Grumiaux - 2021 - SALADNET SELF-ATTENTIVE MULTISOURCE LOCALIZATION .pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\KTSVZ46H\\Grumiaux - 2021 - SALADNET SELF-ATTENTIVE MULTISOURCE LOCALIZATION .pdf:application/pdf},
}

@book{blauert_spatial_1997,
	title = {Spatial hearing: the psychophysics of human sound localization},
	shorttitle = {Spatial {Hearing}},
	abstract = {The field of spatial hearing has exploded in the decade or so since Jens Blauert's classic work on acoustics was first published in English. This revised editio},
	language = {en},
	publisher = {MIT Press},
	author = {Blauert, Jens},
	year = {1997},
	file = {Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\TPK6JUH3\\Spatial-HearingThe-Psychophysics-of-Human-Sound.html:text/html},
}

@article{hawley_benefit_2004,
	title = {The benefit of binaural hearing in a cocktail party: effect of location and type of interferer},
	volume = {115},
	issn = {0001-4966},
	shorttitle = {The benefit of binaural hearing in a cocktail party},
	doi = {10.1121/1.1639908},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hawley, Monica L. and Litovsky, Ruth Y. and Culling, John F.},
	month = feb,
	year = {2004},
	pages = {833--843},
}

@article{arons_review_1992,
	title = {A review of the cocktail party effect},
	volume = {12},
	abstract = {The “cocktail party effect”—the ability to focus one’s listening attention on a single talker among a cacophony of conversations and background noise—has been recognized for some time. This specialized listening ability may be because of characteristics of the human speech production system, the auditory system, or high-level perceptual and language processing. This paper investigates the literature on what is known about the effect, from the original technical descriptions through current research in the areas of auditory streams and spatial display systems.},
	language = {en},
	number = {7},
	journal = {Journal of the American Voice I/O Society},
	author = {Arons, Barry},
	year = {1992},
	pages = {35--50},
	file = {Arons - A Review of The Cocktail Party Effect.pdf:C\:\\Users\\RQML4978\\Zotero\\storage\\L4BFKN2I\\Arons - A Review of The Cocktail Party Effect.pdf:application/pdf},
}

@article{nassif_speech_2019,
	title = {Speech recognition using deep neural networks: a systematic review},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Speech {Recognition} {Using} {Deep} {Neural} {Networks}},
	abstract = {Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.},
	journal = {IEEE Access},
	author = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
	year = {2019},
	pages = {19143--19165},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\9AUJQCS3\\8632885.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\MWJJP4IW\\Nassif et al. - 2019 - Speech Recognition Using Deep Neural Networks A S.pdf:application/pdf},
}

@book{bregman_auditory_1994,
	title = {Auditory scene analysis: the perceptual organization of sound},
	isbn = {978-0-262-52195-6},
	shorttitle = {Auditory {Scene} {Analysis}},
	abstract = {Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of the human auditory system as it analyzes mixtures of sounds to recover descriptions of individual sounds. In a unified and comprehensive way, Bregman establishes a theoretical framework that integrates his findings with an unusually wide range of previous research in psychoacoustics, speech perception, music theory and composition, and computer modeling.},
	language = {en},
	publisher = {MIT Press},
	author = {Bregman, Albert S.},
	year = {1994},
}

@article{tranter_overview_2006,
	title = {An overview of automatic speaker diarization systems},
	volume = {14},
	issn = {1558-7924},
	doi = {10.1109/TASL.2006.878256},
	abstract = {Audio diarization is the process of annotating an input audio channel with information that attributes (possibly overlapping) temporal regions of signal energy to their specific sources. These sources can include particular speakers, music, background noise sources, and other signal source/channel characteristics. Diarization can be used for helping speech recognition, facilitating the searching and indexing of audio archives, and increasing the richness of automatic transcriptions, making them more readable. In this paper, we provide an overview of the approaches currently used in a key area of audio diarization, namely speaker diarization, and discuss their relative merits and limitations. Performances using the different techniques are compared within the framework of the speaker diarization task in the DARPA EARS Rich Transcription evaluations. We also look at how the techniques are being introduced into real broadcast news systems and their portability to other domains and tasks such as meetings and speaker verification},
	number = {5},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Tranter, S.E. and Reynolds, D.A.},
	month = sep,
	year = {2006},
	pages = {1557--1565},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\RQML4978\\Zotero\\storage\\ZM9279H6\\1677976.html:text/html},
}

@article{park_review_2021,
	title = {A review of speaker diarization: recent advances with deep learning},
	shorttitle = {A {Review} of {Speaker} {Diarization}},
	abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
	journal = {arXiv:2101.0962},
	author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
	month = jun,
	year = {2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\XV5YMSYP\\Park et al. - 2021 - A Review of Speaker Diarization Recent Advances w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\PATADHHA\\2101.html:text/html},
}

@article{grumiaux_survey_2021,
	title = {A survey of sound source localization with deep learning methods},
	abstract = {This article is a survey on deep learning methods for single and multiple sound source localization. We are particularly interested in sound source localization in indoor/domestic environment, where reverberation and diffuse noise are present. We provide an exhaustive topography of the neural-based localization literature in this context, organized according to several aspects: the neural network architecture, the type of input features, the output strategy (classification or regression), the types of data used for model training and evaluation, and the model training strategy. This way, an interested reader can easily comprehend the vast panorama of the deep learning-based sound source localization methods. Tables summarizing the literature survey are provided at the end of the paper for a quick search of methods with a given set of target characteristics.},
	author = {Grumiaux, Pierre-Amaury and Kitić, Srđan and Girin, Laurent and Guérin, Alexandre},
	month = sep,
	year = {2021},
	note = {Submitted.},
	file = {arXiv Fulltext PDF:C\:\\Users\\RQML4978\\Zotero\\storage\\KNGDIH8T\\Grumiaux et al. - 2021 - A Survey of Sound Source Localization with Deep Le.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\RQML4978\\Zotero\\storage\\5F6JKMS6\\2109.html:text/html},
}

