\chapter{State-of-the-art on speaker counting and localization}
\label{chap:soa}

\lettrine{I}{n} this chapter, we provide a literature survey addressing speaker counting and localization. We limit the scope of the survey to methods evaluated on speech signals as it was the focus of this thesis. In the Section~\ref{sec:countingLiterature}, we describe the relatively scarce literature on speaker counting, including parametric, clustering and deep learning methods. In Section~\ref{sec:SSLliterature}, we address the literature on sound source localization, by first quickly presenting conventional\footnote{We define as ``conventional'' the methods based on traditional signal processing techniques, without deep learning.} methods, and then providing a short survey of SSL systems using deep learning techniques. As a more exhaustive survey of these neural-based SSL methods has been submitted at the time of this thesis writing \cite{grumiaux_survey_2021}, in this section we limit our description on methods in relation with our research. 

%-----------------------------------------------
%  SPEAKER COUNTING
%-----------------------------------------------
\section{Speaker counting}
\label{sec:countingLiterature}

Speaker counting is the task of estimating the number of people that are speaking in an audio signal. It can be seen as a subtask of speaker diarization, whose objective is to detect which speaker is active at what moment. Most approaches actually focus on predicting the total number of speakers in the analyzed signals, but we found some works considering the instantaneous NoS or the maximum number of simultaneous speakers (see Section~\ref{ss:sourceCounting} for a mathematical definition). Note that in a lot of system, the total and maximum NoS are actually identical since they supposed a constant NoS through the whoel analyzed signal. 

Speaker counting has not often been addressed in the speech processing literature as a separate task, although it is a useful information for other more complex speech-related tasks such as speaker diarization or speech signals separation. Many such systems actually assume the knowledge of the number of speakers, even though we do not have it at hand in practice.

\subsection{Parametric methods}

Early methods for speaker counting attempted to correlate the number of speakers to some features extracted from the audio signal. In \cite{arai_estimating_2003}, the authors proposed to exploit the modulation characteristics of the human voice to correlate the modulation index calculated from the input signal, and the total number of speakers. They computed a function of the modulation index which outputs the number of speakers based on several multi-speaker signals constructed from TIMIT excerpts \cite{garofolo_timit_1993}, containing up to 8 simultaneous speakers. Another parameter, derived from the statistics of a particular mel filter coefficient, has been proposed in \cite{sayoud_proposal_2010} to estimate the total number of speakers in a single-channel mixture. The authors related these statistics directly with the number of speakers via a polynomial function. In \cite{pasha_towards_2017}, a parametric method has been derived for speaker counting, which relies on coherent-to-diffuse ratio estimation over several time frames. The maximum number of speakers $\bar{J}$ is then estimated by thresholding this computed parameter.

\subsection{Clustering methods}

A few counting methods based on clustering algorithms have been proposed in the literature to address source counting along with other tasks, such as source localization or separation. In \cite{arberet_robust_2010}, an algorithm named DEMIX is derived to jointly count, locate and separate up to 6 sources in a multi-channel recording. This method has been applied for speech signals and works in an underdetermined setting, but is however limited to an anechoic environment. Another clustering scheme can be found in \cite{yang_multiple_2017} in which the principal eigenvectors of the covariance matrix of a multi-channel signal are extracted to estimate the total number of sources. It showed to be quite robust for speech recordings with signal-to-noise ratios (SNR) between $0$ and $20$~dB, in environments with low reverberation (TR60 = $250$~ms). In \cite{xu_crowd++:_2013}, the authors use a clustering algorithm to aggregate the mel-frequency cepstrum coefficients (MFCC) computed from successive speech segments extracted from a single-channel audio mixture. Using a cosine similarity, the clustering algorithm compares pairs of MFCC features to aggregate them into a certain number of classes, which corresponds to the estimated total number of speakers.


\subsection{Deep learning methods}

Several recent works have used neural networks to estimate the number of speakers. To the best of our knowledge, the first deep-learning-based method for speaking counting has been proposed in \cite{stoter_classification_2018}. The authors aim to estimate the maximum number of concurrent speakers $\bar{J}$ in a $5$-s single-channel audio mixture, containing at most $10$ speakers. The classification paradigm and the regression paradigm (see Section~\ref{ss:outputScheme} for more details) are compared, using a neural network consisting of bidirectional LSTM layers. The article further evaluates the use of different input features. The presented results indicate the superiority of classification over regression for speaker counting, and that STFT features yield the best performance. The authors extended this work in \cite{stoter_countnet:_2019} by evaluating more neural network architectures, including CNN, RNN and CRNN. They compare the use of usual $3 \times 3$ convolutional kernels with full-band convolutional filters of size $1 \times F$, with $F$ the number of frequency bins. The results indicate that a CRNN with $3 \times 3$ filters leads to the best performance. Their study also includes the use of different datasets, the evaluation of several reverberation time values, and a comparison of their system against human capabilities. 

In the same vein, a speaker counting CNN is proposed in \cite{wei_determining_2018}, capable of categorizing the input signal as containing $1$, $2$ or $3$ and more speakers. In \cite{andrei_overlapped_2019}, an interesting comparison on the human abilities for speaker counting and their machine counterpart is proposed. The authors show that machine algorithms can surpass human level performance, especially when the analysis time is short. Temporal convolutional networks (TCN) have been applied in \cite{cornell_detecting_2020} to count the maximum number of overlapping speakers in a single-channel mixture, as in \cite{stoter_classification_2018}. They show that TCN improves the counting accuracy compared to CRNNs and LSTM-based networks on real data. In \cite{wang_speaker_2020}, a neural-based speaker counting system is trained using transfer learning based on SincNet \cite{ravanelli_speaker_2018}, a speaker recognition network. The output of a truncated version of the already trained SincNet is used as an input feature of their speaker counting system, along with the zero-crossing rate, the spectral spread and the spectral entropy of the input signal. All these features are fed in a feedforward neural network to estimate up to $10$ speakers. Peng \textit{et al.}~\cite{peng_competing_2020} proposed to train a recurrent neural network to project the input features, composed of log-spectra and interaural phase differences (IPDs), extracted from multi-channel signals, into an embedding space. The number of speakers can be obtained as the rank of the covariance matrix of the embedded vectors. An attention-based network is explored in \cite{yousefi_real-time_2021} for speaker counting. After a series of convolutional layers for feature extraction, an attention mechanism is trained to aggregate temporal information in a new feature vector. This vector is then fed into a feedforward layer for the final estimation of the number of speakers.

As estimating the number of speakers is generally done to provide another speech-related task with a precious piece of information, a few works has been proposed to jointly tackle the considered speech processing task and the speaker counting problem. Several speaker diarization/separation systems are trained to simultaneously count and separate speech signals \cite{von_neumann_all-neural_2019,kinoshita_tackling_2020}. These systems do not explicitly estimate the number of speakers, but rather extract the speech signals in a recursive manner. Another joint speaker counting and separation system is proposed in \cite{xiao_improved_2020}, relying on an encoder-decoder architecture. The vector obtained at the bottleneck of the encoder-decoder network is projected into a embedding space, giving a set of embedded vectors whose covariance matrix rank gives the number of sources in the mixture, as in \cite{peng_competing_2020}. In \cite{nguyen_robust_2020}, the authors jointly estimate the number of sources and their respective direction-of-arrivals (DoAs, see Section~\ref{sec:SSLliterature}) by designing a CNN which separates into two distinct branches after a series of convolutional layers: one branch is trained to output the number of sources and another estimate the DoAs. They evaluate their system on speech signals and sound events showing better counting accuracy than DoA-based methods. 


\subsection{Thesis position}

All the above-mentioned and recently proposed DL-based counting methods have shown promising results over conventional methods. Our research for estimating the number of speakers followed this trend. When the present PhD research work was carried out, in the beginning of 2019, the use of neural networks for speech source counting was a pioneering idea and all the related works were limited to single-channel signals. This motivated our effort to improve speaker counting with multi-channel signals in the hope of taking benefit of spatial information in addition to spectral content. Moreover, most systems considered the estimation of the total number of sources $J$, sometimes over audio segments of several seconds, although the instantaneous NoS $J(t)$ often varies along the signal. In our research, we rather focused on this instantaneous NoS $J(t)$. Another aspect that we address was the temporal resolution of the counting system, as most of the speech/audio source counting literature, at the time of our experiments, considered a quite large temporal context, which could be insufficient for online systems or for applications that require a good temporal resolution, such as speech signals separation. Finally, we made use of Ambisonics features, as for all the remaining of this thesis, which had never been proposed in the counting literature. The interest of such signal representation has been discussed in Chapter~\ref{chap:ambisonics} and we assumed that it would be also appropriate for the speaker counting task since spatial information can help to detect spatially distinct speakers.

%-----------------------------------------------
%  SOUND SOURCE LOCALIZATION
%-----------------------------------------------
\section{Sound source localization}
\label{sec:SSLliterature}

The sound source localization problem has been thoroughly studied for decades, often with a focus on speech signals. A large variety of methods have been designed to address SSL in different scenarios, in anechoic and reverberant conditions, considering one or more sources, static or moving in the environment. In this section, after first presenting a short description of conventional SSL methods based on signal processing, we focus on deep-learning-based SSL techniques, as these have recently become a hot topic in the literature. We put an emphasis on neural SSL systems related to this thesis research.

\subsection{Traditional signal processing methods}

\subsubsection{Time difference of arrival}

When multiple microphones are used to record a sound field, the signal incoming from a point source arrives at different instants at each microphone. The time difference of arrival (TDoA), between pairs of microphones, contains information about the direction of the source if the arrangement of the microphones is known. The TDoA can be estimated as the time lag corresponding to the maximum of the cross-correlation function between a pair of microphones. However, in real conditions it is blurred by noise and reverberation. To improve the robustness of this technique, Knapp and Carter \cite{knapp_generalized_1976} introduced in 1976 \textit{generalized cross-correlation with phase transform} (GCC-PHAT), which is computed by dividing the cross-correlation by its amplitude. This can be computed using the signals of a microphone pair $(i,i')$, for a time frame $t$ and a lag $\tau$, by summing over all frequencies $f$:
\begin{equation}
\label{eq:gcc_phat}
    \Psi_{ii'}(t,\tau) = \frac{1}{F} \sum_{f=0}^{F-1} \frac{x_i(t,f) x^{*}_{i'}(t,f)}{\mid x_i(t,f) \mid \mid x_{i'}(t,f) \mid} e^{2 i \pi \tau \frac{f}{F}},
\end{equation}
where $F$ is the total number of  frequencies, and $x_i(t,f)$ and $x_{i'}(t,f)$ are the signals in the STFT domain from microphones $i$ and $i'$, respectively.
Then the corresponding TDoA $\Delta_{ii'}(t)$ can be deduced as:
\begin{equation}
    \Delta_{ii'}(t) = \argmax_{\tau} \Psi_{ii'}(t,\tau).
\end{equation}
Because of the demoninator in \eqref{eq:gcc_phat}, the GCC-PHAT method is quite sensitive to noise, and besides is poorly robust in the presence of multiple sources \cite{blandin_multi-source_2012}.

Many neural-based SSL methods rely on GCC-PHAT features as input for a localization neural network \cite{xiao_learning-based_2015, vesperini_neural_2016, li_online_2018, comanducci_source_2020, vera-diaz_towards_2021} (see Section~\ref{ss:neuralSSL} for a more detailed survey of SSL with neural networks).

\subsubsection{Acoustic maps}

Another way to localize sound sources is to generate \textit{acoustic maps}, which relate the energy or power of acoustic signals to predefined search directions (\emph{e.g.}, on a discrete grid). The steered response power (SRP) can be used to generate such maps, and similarly to GCC-PHAT, it has also been adapted with the phase transform, resulting into the SRP-PHAT algorithm \cite{dmochowski_generalized_2007}. Such an acoustic map can be obtained by scanning the whole space with a beamformer and computing the signal energy or power for each considered direction.

Because of the amplitude normalization and the averaging across all microphone pairs, this method is more robust to reverberation, however it has the undesired effect of emphasizing time-frequency bins containing only noise.

Neural networks have been used to improve the robustness of the SRP-PHAT algorithm. In \cite{pertila_robust_2017}, a CNN has been trained to estimate a time-frequency (TF) mask for each microphone signal. These are applied to corresponding STFT representations, prior to computing the GCC-PHAT quantity of each microphone pair. This method shows to be more robust to strong interfering sources and reverberation. Another system, proposed by Diaz-Guerra et al. \cite{diaz-guerra_robust_2021}, relies on a CNN to directly estimate the Cartesian coordinates of a sound source from an SRP-PHAT acoustic map, again improving the performance in reverberant and noisy conditions.

\subsubsection{Subspace methods}

Subspace methods are based on the eigendecomposition of the spatial covariance matrix (SCM) calculated from the observation vectors. In the narrowband \textit{multiple signal classification} (MUSIC) algorithm \cite{schmidt_multiple_1986}, a subset of eigenvectors obtained from the decomposition is attributed to the sources, while the complementary set of eigenvectors is the basis of the noise subspace. The latter are used to compute an acoustic map:
\begin{equation}
    \mathcal{M}(t,f,\theta,\phi) = \frac{1}{\mathbf{a}_{\theta,\phi}^H(f) \mathbf{U}_N(t,f) \mathbf{U}_N^H(t,f) \mathbf{a}_{\theta,\phi}(f)},
\end{equation}
where $\mathbf{U}_N$ is a matrix that contains the noise-related eigenvectors, and $\mathbf{a}_{\theta,\phi}(f)$ is the \textit{steering} vector in direction $(\theta,\phi)$, which represents the delays of the different captures of a plane wave recording at a microphone array. The estimated DoA is obtained with the angles $(\theta,\phi)$ that maximize this acoustic map, thus one needs to probe all considered directions. This time-demanding search can be avoided with another well-known subspace algorithm for source localization, called the estimation of signal parameters via rotational invariance techniques (ESPRIT) \cite{roy_esprit-estimation_1989}, which relies on the source subspace to directly estimate the DoA.

\subsection{Deep learning techniques}
\label{ss:neuralSSL}

In this subsection, we describe different systems that can be found in the DL-based sound source localization literature. We categorize these systems with regards to several aspects: number of sources to localize, network architecture, type of input features, output scheme, learning strategies and training/test datasets. This part of the manuscript is a shortened version of the comprehensive survey of the neural-based literature that we have written and submitted for publication (a preprint version is available \cite{grumiaux_survey_2021}).


\subsubsection{Number of sources}

Many neural-based SSL systems consider only one source to localize, as it is already a very complex problem in real-world environments, due to the presence of noise and reverberation. When the source activity is not controlled artificially, some methods rely on a VAD system as a preprocessing step before localization. It is also possible to simultaneously estimate the source activity \emph{and} perform localization, as in \cite{yalta_sound_2017}. In this work, an additional neuron is appended to the network output, and used to estimate whether the source is active or not. Another way of estimating the NoS alongside localization is to adopt a thresholding method, notably when using a classification paradigm (see Section~\ref{ss:outputScheme}).

Localizing multiple sources is a much harder problem than single-source localization, especially when the activity of the different sources overlaps in time, as we illustrated in Chapter~\ref{chap:introduction}. Nowadays, more and more neural-based localization systems attempt to improve multi-source SSL performance in noisy and reverberant environments. As in the single-source case, many of these multi-source methods assume the NoS $J$ (as described in Section~\ref{ss:sourceCounting}) is known before estimating their locations \cite{hirvonen_classication_2015, chakrabarty_multi-speaker_2017, ma_phased_2018, perotin_crnn-based_2019}, which is then used to estimate the right number of DoAs. In practice, $J$ can be estimated by a dedicated source counting algorithm. An alternative, proposed in a few articles, is to jointly estimate the total number of sources $J$ and their location, either from the localization output \cite{he_joint_2018,moing_learning_2020,sundar_raw_2020}, or by designing a multi-task network trained to also explicitly estimate the NoS \cite{nguyen_robust_2020}. Note that in all these works, the sources are supposed to be static and the NoS is constant, \emph{i.e.}, $J(t) = J, \forall t$.

\subsubsection{Input features}

Many types of input features have been used in the neural SSL literature. Some systems are inspired by signal representations from conventional methods. For instance, GCC-PHAT features have been employed in \cite{xiao_learning-based_2015, vesperini_neural_2016, he_deep_2018, comanducci_source_2020}, while SRP-PHAT-based acoustic maps have been used in \cite{salvati_exploiting_2018, diaz-guerra_robust_2021}. Other features based on cross-correlation functions have been proposed in \cite{grondin_sound_2019, ma_phased_2018}. Ideas from subspace methods have also been exploited in several works. For instance, the eigenvectors of the SCM are fed into neural networks in \cite{takeda_discriminative_2016, takeda_unsupervised_2018}, while in \cite{nguyen_robust_2020} the authors exploit the spatial pseudo-spectrum from the MUSIC algorithm.

Other classical types of features, usually employed in conventional methods, have been reused as input for neural networks. In \cite{chazan_multi-microphone_2019, bianco_semi-supervised_2020}, the authors proposed to compute the relative transfer function (RTF) obtained from all microphone pairs and fed it into the neural network. Other neural SSL systems were designed and used in a binaural set-up, which is a two-microphone format designed to mimic human listening conditions. They thus used classical binaural cues as input features of the network: inter-aural level differences \cite{youssef_learning-based_2013, roden_sound_2015, zermini_deep_2016}, inter-aural time differences \cite{youssef_learning-based_2013, roden_sound_2015}, and  inter-aural phase differences \cite{pak_sound_2019, nguyen_autonomous_2018, sivasankaran_keyword-based_2018, shimada_accdoa_2020, subramanian_deep_2021}.

Low-level representations have also been investigated as neural network inputs, letting the model learn to extract from them the relevant information for localization during the training phase. A number of SSL neural networks proposed in the literature rely on (multi-channel) STFT spectrograms. The network can use only the STFT magnitude \cite{yalta_sound_2017, pertila_robust_2017}, only the STFT phase \cite{subramanian_deep_2021, zhang_robust_2019}, or both \cite{guirguis_seld-tcn_2021, krause_comparison_2021, maruri_gcc-phat_2019, schymura_pilot_2021}. Some systems use the decomposition of complex-valued spectrograms into real and imaginary parts \cite{he_joint_2018, moing_learning_2020}. Finally, a few \textit{end-to-end} neural networks have been designed to estimate the source location directly from the raw multi-channel waveforms. In \cite{suvorov_deep_2018}, the waveforms are fed into 1D convolutional layers, while in other systems \cite{vera-diaz_towards_2018, vecchiotti_end--end_2019, pujol_beamlearning_2021} 2D convolutional layers are preferred.

Finally, many authors chose to take benefit of the Ambisonics format (see Chapter~\ref{chap:ambisonics}) for neural-based SSL. Some of them proposed to feed the network with an Ambisonics spectrogram \cite{adavanne_localization_2019, guirguis_seld-tcn_2021, schymura_exploiting_2021}, while in \cite{comminiello_quaternion_2019} such spectrogram is considered as quaternion-valued and the authors adapted the neural network to operate on such features. The intensity vector is another Ambisonics-based representation that has proven effective for neural-based SSL, according to several articles \cite{perotin_crnn-based_2018, perotin_crnn-based_2019, nguyen_general_2021}. Regarding the Ambisonics order, most of these methods work with the FOA format, but we can find some systems based on the HOA features \cite{varanasi_deep_2020,poschadel_direction_2021}.

\subsubsection{Architectures}

Neural network architectures are probably the most explored aspect of DL-based SSL systems.  The early deep learning SSL approaches employed simple feedforward neural networks 
\cite{kim_direction_2011,youssef_learning-based_2013,xiao_learning-based_2015,vesperini_neural_2016,roden_sound_2015}.
CNNs have also been applied early to SSL, having been proven very powerful for computer vision tasks. The first use of a CNN for SSL can be found in \cite{hirvonen_classication_2015}, with the model based on 2D convolutional layers. Many other works also employed convolutional layers \cite{chakrabarty_multi-speaker_2017, chakrabarty_multi-speaker_2019, he_joint_2018, vera-diaz_towards_2018}. An architecture with 1D convolutions layers has been proposed in \cite{bologni_acoustic_2021}, while 3D convolutions are used in \cite{diaz-guerra_robust_2021}. Moreover, dilated convolutions have also been explored in several papers \cite{chakrabarty_multi-speaker_2019,pujol_beamlearning_2021,guirguis_seld-tcn_2021}. In \cite{chakrabarty_multi-speaker_2019} the authors interestingly show that using the dilated convolutional layers with the increasingly larger dilation rates allows to reduce the total number of layers. A comparison of several type of convolutional layers can be found in \cite{krause_comparison_2021}.

While the architectures consisting only of recurrent layers are rare in the SSL literature, we find a lot of works which consider CRNNs, whose convolutional part is generally useful for feature extraction, and recurrent layers are employed for temporal analysis. Examples of CRNN-based SSL systems can be found in \cite{adavanne_localization_2019,perotin_crnn-based_2018,perotin_crnn-based_2019,maruri_gcc-phat_2019,comminiello_quaternion_2019}.

Inspired again by the architectures from the computer vision literature, some neural SSL models incorporate residual connections, such as the networks proposed in  \cite{yalta_sound_2017,suvorov_deep_2018}.

Attention mechanisms, which impressively improved NLP models, have also been employed in a few SSL neural networks. In \cite{schymura_exploiting_2021, phan_multitask_2020}, the authors added attention layers to the end of a CRNN, resulting in a better use of temporal information in DoA estimation. Self-attention has also been integrated after a series of convolutional layers in \cite{cao_improved_2021, schymura_pilot_2021, wang_four-stage_2021}.

Finally, the use of encoder-decoder architecture for SSL has been explored in several works, for example in \cite{huang_time-domain_2020, moing_learning_2020, wu_sslide_2021}.

\subsubsection{Output strategies}
\label{ss:outputScheme}

When addressing SSL with neural networks, two ways of designing the output layer are essentially considered, corresponding to the two following formulations of the SSL problem: classification and regression. 

When considering SSL as a multi-label classification problem, the analyzed space is divided into a grid with many subregions (corresponding to different classes), and the network is actually trained to detect the presence of a source in each subregion, by outputting a presence probability. Theoretically, a very large number of sources can be detected, depending on the grid resolution. The main advantage of this approach is that it is possible to localize any number of sources. By setting the coordinate system origin as the microphone position, a suitable way of describing the surrounding space is to use spherical coordinates $(\theta,\phi,r)$, denoting the azimuth, elevation and distance (range) of a sound source. In the literature we can find a lot of neural SSL systems designed to estimate only the azimuth \cite{roden_sound_2015,hirvonen_classication_2015,suvorov_deep_2018,vecchiotti_end--end_2019,xiao_learning-based_2015,chazan_multi-microphone_2019}, only the elevation \cite{thuillier_spatial_2018}, or both \cite{perotin_crnn-based_2018,perotin_crnn-based_2019,adavanne_localization_2019}, while only few works addressed distance estimation \cite{roden_sound_2015,bologni_acoustic_2021}. Cartesian coordinates $(x,y,z)$ have also been considered, though more rarely, using a classification paradigm, however limited to the estimation of $(x,y)$ only \cite{moing_learning_2020,ma_phased_2018}.

Regression is another paradigm with which the network is trained to directly estimate the coordinates of a certain number of sources. To do so, each source coordinate is represented by one neuron whose value directly encodes the considered coordinate. One advantage of this approach is to not relying on a grid to represent the sound space. However a limitation occurs when considering multiple sources because of the source permutation problem \cite{subramanian_deep_2021}, which deals with the ambiguity in the association between target and actual output. Despite this drawback, regression has been widely used in neural-based SSL. With spherical coordinates, some systems estimate only the azimuth \cite{nguyen_autonomous_2018,opochinsky_deep_2019}, while others estimate both azimuth and elevation \cite{maruri_gcc-phat_2019,sundar_raw_2020}. However, most regression-based methods are trained to estimate cartesian coordinates, for example in \cite{vera-diaz_towards_2018,krause_comparison_2021,adavanne_localization_2019,comminiello_quaternion_2019}.

\subsubsection{Data}

When dealing with deep learning methods, the choice for training and testing data is very important. While the ideal case would be to train a neural network with a large amount of real-world data, in practice only a few real-world datasets annotated with the source locations are available, and the amount of such labelled data remains limited. That is why simulated data are employed in most neural systems during the training phase. 

The most common approach to generate somewhat realistic multi-channel signals is by using artificial room impulse responses. These take into account the room acoustics as well as the position of the sources and microphone array in the environment. While several methods exist to simulate such IRs \cite{svensson_computational_2002}, the image source method (ISM) \cite{allen_image_1979} is undoubtedly the most employed in the neural SSL literature. It has been implemented in several publicly available frameworks, such as RIR generator \cite{habets_room_2006}, SMIR generator \cite{jarrett_rigid_2012}, Pyroomacoustics \cite{scheibler_pyroomacoustics_2018} and McRoomSim \cite{wabnitz_room_2010}. Such frameworks have been employed in \cite{chakrabarty_multi-speaker_2017,perotin_crnn-based_2018,nguyen_robust_2020,varanasi_deep_2020,salvati_exploiting_2018} to name a few. Other simulation methods have been explored, for instance in \cite{hirvonen_classication_2015} in which a diffuse reverberation model is added to the ISM, or in \cite{gelderblom_synthetic_2021} where the authors compare several synthesis algorithms.

When an IR is simulated, it is then convolved with a ``dry'' speech signal (clean, monophonic and obtained with close-mike recording in a low reverberation environment) in order to obtain a realistic signal which encodes the room acoustics and the propagation between the signal source and the microphone array. Among the speech signal datasets used in the SSL literature, one can cite TIMIT \cite{garofolo_timit_1993}, BREF \cite{larnel_bref_1991} or WSJ \cite{garofolo_csr-i_2007}.

Regarding real-world data, a few datasets are available and are generally used to evaluate the neural systems. Recorded IR datasets \cite{cristoforetti_dirha_2014, francombe_iosr_2017, hadad_multichannel_2014} have been collected to further generate more realistic signals. A few other databases of signals recorded in real environments along with the source locations are also publicly available \cite{politis_dataset_2021, evers_locata_2020, guizzo_l3das21_2021}.

\subsubsection{Learning strategies}

Another important aspect of DL-based methods which varies among the SSL literature is the choice of learning strategy. When a sufficient amount of labelled data is available, a neural network can be trained with supervised learning. It is the most employed training strategy in the neural SSL literature, despite the fact that the amount of labelled real-world recordings is limited, owing to the use of simulated data. Examples of SSL systems relying on supervised learning can be found in \cite{perotin_crnn-based_2018,youssef_learning-based_2013,hirvonen_classication_2015,chazan_multi-microphone_2019,chakrabarty_multi-speaker_2019}.

Semi-supervised learning has also drawn interest in a few works on neural SSL systems. Such a learning scheme relies on an initial supervised training phase (using a limited amount of labelled data), followed by another training phase using (a possibly larger amount of) unlabelled data. For instance, in \cite{takeda_unsupervised_2018,moing_data-efficient_2021} the unlabelled data are used to adapt a neural network, pre-trained with labelled data, to unseen conditions.

Another learning strategy, termed weakly supervised training, aims to train a neural network with weak labels, \emph{i.e.}, the labels that can be inaccurate, imprecise or containing ``coarse'' meta-information. In \cite{he_adaptation_2019}, the NoS is used as weak labels to further train the network using an adapted loss function. Another example of weakly supervised learning can be found in \cite{opochinsky_deep_2019}, in which the authors proposed to use a triplet loss function, which relies on adding two other examples to an usual example: a \textit{positive} example which is close in the localization space to the usual example, and a \textit{negative} example which is far away in the space. The interest of this scheme is that it can be used with only a few labelled data, providing that we can control the proximity of unlabelled data in the target space.

\subsection{Thesis position}

As for speaker counting, DL-based systems are more and more often employed in the SSL literature, as they show to be more robust to challenging conditions, such as noise, reverberation or the presence of multiple sources, than traditional methods. This thesis work also focused on using neural networks for SSL, based on the research initiated in the earlier PhD thesis \cite{perotin_localisation_2019}. At the time of our source localization experiments, single-source localization using neural networks on real-world data was already quite efficient. Using the versatility of the Ambisonics format, particularly the intensity vector, in this thesis we focused on the multi-source localization problem, which was still poorly addressed compared to the single-source configuration. We took an interest in rethinking several architecture blocks, in order to improve the localization performance. In the same vein as our effort to reduce the temporal resolution for speaker counting models, we also focused on reducing the computation time of SSL neural networks. Finally, in a series of exploratory experiments, we tried improving single-source localization with a novel Ambisonics representation, which has never been considered in the neural-based SSL literature.
