\chapter{Conclusion}
\label{chap:conclusion}

%-----------------------------------------------
%  CONCLUSION
%-----------------------------------------------
\section{Conclusion}

In this thesis, we focused on estimating the number of speakers and their respective locations from a multi-channel audio signal. We considered the challenging context of domestic environments, where reverberation and noise are present. We addressed these problems with deep neural networks, which were shown to be able to tackle the speaker counting and localization problems efficiently, with an emphasis on low-latency performance.

\subsection{Speaker counting}

In the first part, we trained a CRNN on simulated data to count the number of speakers in multi-channel mixtures which are blurred by reverberation and diffuse noise. We showed that this counting system was capable of counting up to $5$ speakers, at a frame-wise resolution, while the best speaker counting method at the time of this research was able to provide the maximum NoS in a $5$-s audio segment. Our method proved to be quite robust, with a very high accuracy when estimating the NoS to be $0$ to $2$, while this accuracy is still above $50$\% for greater NoS. After showing the benefit of using multi-channel features over single-channel ones, we conducted several experiments to analyze the performance according to several sets of hyperparameters. Finally, we carried on an investigation to assess the importance of a frame position within the input sequence to maximize the corresponding output accuracy. We derived an empirical formula which gives the frame position in the input sequence having the most accurate NoS estimate, leading to an overall accuracy of more than $75$\%. This short analysis was not bound to our counting problem, but is rather generic, based on the intuition behind the functioning of convolutional and LSTM layers.

\subsection{Speaker localization}

In the second part, we addressed the speaker localization problem, under the assumption that the NoS is preliminary known. We first considered the simpler problem of localizing one speaker, in order to assess the interest of the TDVV, a novel input feature with promising theoretical perspectives. In order to improve the single-speaker localization performance over the use of the intensity-based input features, we explored a number of neural network architectures, such as CRNNs, CRNNs with dilated convolutions and CRNNs with residual connections. Nevertheless, the results never showed an improvement over the baseline, and yet it is quite difficult to analyze the neural network to understand the reasons for this behavior. In spite of the efforts to derive an adapted feature extraction module for the TDVV, we concluded that the problem could lie into the TDVV estimation itself. 

Then, we focused on estimating the DoAs of multiple overlapping speakers with intensity-based features. These had already proven to be quite robust for single-speaker localization, and are deemed promising for localizing multiple speakers as well. After evaluating the best way to train the localization neural network for multiple speakers, we redesign the feature extraction module of a state-of-the-art model in order to give more capacity to the network to derive effective representations. This new design has greatly improved the localization performance, especially in a multi-speaker context. Next, we proposed to replace the classical recurrent layers present in the CRNN with self-attention mechanism. The objective was to reduce the inference time while retaining similar localization performance, which was successfully accomplished. We further managed to improve the multi-speaker localization performance by proposing a new way of computing attention scores with multiple heads. Finally, we assessed the benefit of using the HOA-, instead of the FOA-based features. The obtained results indicated that the network trained using higher-order pseudointensity is performing better than its FOA counterpart, particularly in the presence of multiple speakers.

\subsection{Joint speaker counting and localization}

In the last part, we proposed to relax the assumption that the NoS needs to be known beforehand, by combining our speaker counting and speaker localization systems. First, we evaluated the robustness of using the speaker counting network to estimate the NoS instead of using the ground-truth. We demonstrated that the counting system is accurate enough so that the localization network leads to an estimate of most DoAs with sufficient efficiency. Then, we investigated the idea of injecting the NoS information as an additional input feature into the localization network. We observed that when the injection took place at specific layers, \emph{i.e.}, after the feature extraction and temporal analysis modules, it helped the localization network to be more accurate. Finally, we jointly addressed speaker counting and localization with a common multi-task neural network. After evaluating the best way to combine the counting and localization loss functions, we designed several multi-task neural network architectures and compared them with the use of separate counting and localization systems. We demonstrated that these multi-task networks are more robust in terms of counting accuracy than our original speaker counting, and that the localization performance remained high. These last experiments suggested that a neural model can simultaneously perform speaker counting and localization, without the strong assumption of knowing the NoS in advance.

%-----------------------------------------------
%  PERSPECTIVES
%-----------------------------------------------
\section{Perspectives}

In this thesis, we focused on speaker counting and localization from many angles, especially regarding the design of several network modules with a goal of improving the overall performance. However, our experiments and the subsequent results showed some limitations which we did not have time to address. In the following paragraphs, we discuss a few perspectives and ideas which would be interesting to develop in order to improve the networks performance, and gain a better understanding of their inherent behavior.

\subsection{Real-world data adaptation}

In all our experiments we noticed a drop in performance when the neural networks, which had always been trained on simulated data, were tested against real-world signals. This effect is well-known in the deep learning research, and is quite usual in many tasks due to the lack of realistic training data. In our research, all training examples were generated using the ``shoebox'' acoustic simulations. Such geometry is rarely encountered is real-world environments. Moreover, the microphone array was allowed to be placed (almost) anywhere in the room, resulting in unrealistic positions (\emph{e.g.}, floating in the air), whereas in reality a recording device is often positioned on a table, leading to strong reflections. A first approach is to consider a more sophisticated room acoustics simulator, capable of taking into account more complex room geometries and acoustic phenomena, such as scattering or diffraction.
Such an idea, however, presents the limitation of a heavier computation cost, which should be balanced with the amount of data to be generated. Another idea would be to progressively train the network to more and more realistic signals, \emph{e.g.}, first with signals generated with simulated SRIRs, then fine-tuning the network with signals generated with real SRIRs, then again fine-tuning it further using recorded data. Finally, one could lean towards the research effort on domain adaptation, which provides interesting methods to improve the performance of a network on a particular domain (\emph{e.g.}, real-world data in our case) while it has been trained on another domain (\emph{e.g.}, simulated data).

\subsection{Neural networks process analysis}

Although some efforts were made in this thesis to analyze the networks behavior and the influence of some hyperparameters, there is still room for in-depth analyses, which would certainly be profitable to understand why some models perform better than others, and how further improvements can be achieved. This analysis concern is at the core of recent deep learning research, because of the neural network's black box nature, and we hereby propose a few targeted points to be analyzed regarding our domain. A first perspective is to interpret the extracted features which allows the networks to perform quite well in a multi-speaker context, for example based on filter visualization techniques \cite{chollet_deep_2017,bach_pixel-wise_2015}. Regarding the improvements obtained in Section~\ref{ss:multiLocalizationfeatureExtractionModule}, one can wonder what are the differences between the extracted feature of the baseline and the one of the new module which leads to such a performance gain. Also, the small variations in the results for the different proposed feature extraction modules push us into looking into the networks behavior when another convolutional block is added or less max-pooling is employed. In the same vein, it could be very instructive to study the contents of the extracted feature in the multi-task networks, since it proved to be suitable not only for localization but also for counting. While it is obvious that these two tasks are bound in a certain way, the networks seem to extract meaningful patterns in the input features which are somehow beneficial for counting and localization altogether, which is not straightforward to do with our human-crafted algorithms.

Furthermore, it could be quite insightful to understand how the attention mechanism is able to make use of the inter-frame information. While the behaviors of LSTM layers are a bit more instinctive to understand due to their recurrent processing, it is more complicated to figure out how self-attention scores are computed, and why the emphasis is put on certain frames. A thorough investigation regarding the scores and the computed vectors could reveal interesting properties on the most informative frames for speaker localization. 
We believe that an extensive analysis effort should be pursued to definitely leverage the remarkable capabilities of neural networks. Not only could it allow to notably improve the accuracy of counting and localization systems, but it could more importantly provide us new considerations to progress in the audio research field.

\subsection{Moving sources}

In some experiments, we evaluated our localization network on the LOCATA dataset, which includes two sets of data with moving speakers. While we never consider this particular context, we saw that our localization system was able to follow the DoAs of several speakers with a fair accuracy, which is probably due to a relatively short temporal analysis window. However, the localization performance could be greatly improved with a dedicated tracking system. We could leverage our robust speaker counting system to provide an information about the sources ``birth'' or ``death'', alongside a neural-based tracking network, to improve the localization of moving sources. Furthermore, it could be interesting to incorporate speaker spectral signatures into a tracking system for even better performance, which would bring us at the intersection with speaker recognition and diarization.

\subsection{Combination of deep learning and conventional signal processing techniques}

In this PhD work, the exploitation of the spatial information contained in the mixture signal was done mostly ``implicitly,'' by inputing a multichannel (Ambisonic) representation of the signal in the DNNs. Another perspective that we have not considered in this PhD work is in the ``explicit'' combination of deep learning with conventional multichannel signal processing techniques, inspired by what has been done for speech enhancement and speech/audio source separation. 

Deep-learning-based (single-channel) speech enhancement and separation are mostly based on the masking approach in the TF domain. Binary masks or soft masks (reminiscent of the well-known single-channel Wiener filter) are estimated with DNNs from the noisy signal and applied to it to obtain a cleaned version \cite{wang2017supervised}.
For multichannel speech enhancement and separation, it is possible to input the multichannel signal in the mask estimation network. But, it can been noted that the masking technique has also been combined with conventional (multichannel signal processing) approaches such as beamforming \cite{van1988beamforming}, see for example \cite{erdogan2016improved}, \cite{heymann2016neural} and \cite{higuchi2017deep}. The authors of these three papers proposed a similar basic combination of DL-based single-channel speech enhancement (to extract/exploit the speech spectral information) and beamforming techniques (to extract/exploit the spatial information). DNNs are used to estimate masks in the TF domain, which are used to select speech-dominant against noise-dominant TF points. The speech-dominant and noise-dominant points are then used to estimate speech and noise spatial covariance matrices, respectively, which are then used to build beamforming filters. Those studies report better ASR scores than with direct TF masking or basic beamforming applied separately.\footnote{Note that, in parallel, more direct and brute-force approaches were also considered, with joint end-to-end optimization of the mask estimator, the beamformer, and possibly an ASR acoustic model, in the TF domain \cite{meng2017deep, heymann2017beamnet}, and in the time domain \cite{li2016neural}.}
This approach was extended in \cite{perotin2018multichannel} with an additional first stage of beamforming in the high-order ambisonics domain to improve the mask estimation. 

In the same general idea, but an approach closer to source separation than to beamforming, the authors of 
\cite{nugraha2016multichannel} combined a DNN trained to estimate a clean speech spectrogram from a noisy speech spectrogram with the source separation technique based on the spatial covariance matrix (SCM) model and Wiener filtering from \cite{duong2010un}. An unsupervised multichannel speech enhancement system combining a deep model (a variational autoencoder) for modeling the clean speech signal and the SCM model from \cite{duong2010un} for modeling the spatial characteristics was proposed in \cite{leglaive2019semi}. 

All these work lead us to believe that there is room for combining DNNs and conventional models (especially for the modeling of spatial information) for source counting and localization, as well as for combining these tasks with speaker diarization, enhancement and separation. The connection between audio source separation and SSL is strong, reciprocal (audio source separation can help  SSL and SSL can help audio source separation), and is already exploited in many studies \cite{vincent_audio_2018, gannot2017consolidated}. Obviously, a reliable NoS estimation is also useful for both SSL and separation. Therefore, a straighforward extension of our work presented in Section~\ref{sec:Multi-task-counting-loc} could be to add the separation task at the output of the proposed speaker counting / speaker localization multi-task network. And beyond that, future works may consider jointly source counting, localization, diarization and separation/enhancement in an hybrid approach combining deep learning and conventional signal processing techniques.

